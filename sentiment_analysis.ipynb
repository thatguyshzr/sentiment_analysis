{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkBbFmtttgRp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.67158031463623\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start= time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgyg4kPR-_PH"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"data\\train.csv\")\n",
    "test=pd.read_csv(r\"data\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "ZZgqIaJK_U7e",
    "outputId": "0ab66e43-56d1-4870-e818-486965a5cc6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "lrGI7cZE_YJd",
    "outputId": "839514ff-a0e2-448b-db84-48841c0a5fb5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7506</td>\n",
       "      <td>Audience Q: What prototyping tools do you use?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7992</td>\n",
       "      <td>At SXSW? Send Your Best Photos &amp;amp; Videos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>@mention  and here's a pic of you winning your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7688</td>\n",
       "      <td>Google Marissa Mayer: mobile phone as a cursor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>#SXSW Google maps is even cooler than I thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet\n",
       "0      7506  Audience Q: What prototyping tools do you use?...\n",
       "1      7992  At SXSW? Send Your Best Photos &amp; Videos to...\n",
       "2       247  @mention  and here's a pic of you winning your...\n",
       "3      7688  Google Marissa Mayer: mobile phone as a cursor...\n",
       "4      3294    #SXSW Google maps is even cooler than I thought"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_DjOBFLDU9z"
   },
   "outputs": [],
   "source": [
    "def preprocessing_text(table):\n",
    "    #put everything in lowercase\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    #Replace rt indicating that was a retweet\n",
    "    table['tweet'] = table['tweet'].str.replace('rt', '')\n",
    "    #Replace occurences of mentioning @UserNames\n",
    "    table['tweet'] = table['tweet'].replace(r'@[^\\s]+', 'AT_USER', regex=True)\n",
    "    #Replace links contained in the tweet\n",
    "    table['tweet'] = table['tweet'].replace(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', regex=True)\n",
    "    #remove numbers\n",
    "    table['tweet'] = table['tweet'].replace(r'[0-9]+', '', regex=True)\n",
    "    #replace special characters and puntuation marks\n",
    "    table['tweet'] = table['tweet'].replace(r'[!\"$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
    "    table['tweet'] = table['tweet'].replace(r'#([^\\s]+)', r'\\1', regex=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "Lpz6Lm4E-AGF",
    "outputId": "6e5cf51a-1cf4-407b-fd2e-4911c4981359",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shahzar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shahzar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shahzar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shahzar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FduoxfLQhZ1P"
   },
   "outputs": [],
   "source": [
    "def stop_words(table):\n",
    "    #We need to remove the stop words\n",
    "    stop_words_list = stopwords.words('english') + ['sxsw']\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8NSQaPpqLM8"
   },
   "outputs": [],
   "source": [
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BK3G5hDhftG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def replace_antonyms(word):\n",
    "    #We get all the lemma for the word\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for lemma in syn.lemmas(): \n",
    "            #if the lemma is an antonyms of the word\n",
    "            if lemma.antonyms(): \n",
    "                #we return the antonym\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hEIY2Qphl84"
   },
   "outputs": [],
   "source": [
    "def handling_negation(row):\n",
    "    #Tokenize the row\n",
    "    words = word_tokenize(row)\n",
    "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
    "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
    "    tags = nltk.pos_tag(words)\n",
    "    #Now we ask if we found a negation in the words\n",
    "    tags_2 = ''\n",
    "    if \"n't\" in words and \"not\" in words:\n",
    "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
    "        words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
    "    elif \"n't\" in words:\n",
    "        tags_2 = tags[words.index(\"n't\"):]\n",
    "        words_2 = words[words.index(\"n't\"):] \n",
    "        words = words[:words.index(\"n't\")+1]\n",
    "    elif \"not\" in words:\n",
    "        tags_2 = tags[words.index(\"not\"):]\n",
    "        words_2 = words[words.index(\"not\"):]\n",
    "        words = words[:words.index(\"not\")+1] \n",
    "        \n",
    "    for index, word_tag in enumerate(tags_2):\n",
    "        if word_tag[1] in speach_tags:\n",
    "            words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
    "            break\n",
    "            \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVyD7zhriiJC"
   },
   "outputs": [],
   "source": [
    "def cleaning_table(table):\n",
    "    #This function will process all the required cleaning for the text in our tweets\n",
    "    table = preprocessing_text(table)\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: detect_elongated_words(x))\n",
    "    table['tweet'] = table['tweet'].apply(lambda x: handling_negation(x))\n",
    "    table = stop_words(table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "gsWnNu-2qs7W",
    "outputId": "6ecacaed-2924-4956-8fdf-31b00d166563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7273 entries, 0 to 7273\n",
      "Data columns (total 3 columns):\n",
      "tweet_id     7273 non-null int64\n",
      "tweet        7273 non-null object\n",
      "sentiment    7273 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 227.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_SOnjdOqw2U"
   },
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpO1dmktrWNL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.03087592124939\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "train = cleaning_table(train)\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFtgOOTTtItk"
   },
   "outputs": [],
   "source": [
    "test = cleaning_table(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "a6iMMto3Rqua",
    "outputId": "34cab73f-bbc7-4521-b86c-88be5addf95b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>sxswnui apple defining language touch differen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>learning ab google doodles doodles light funny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one inyourface ex stealing show yrs atuser quo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>iphone ap would b pretty awesome n't crash ext...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>line outside apple store austin waiting new ip...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  sxswnui apple defining language touch differen...          1\n",
       "1      1851  learning ab google doodles doodles light funny...          1\n",
       "2      2689  one inyourface ex stealing show yrs atuser quo...          2\n",
       "3      4525  iphone ap would b pretty awesome n't crash ext...          0\n",
       "4      3604  line outside apple store austin waiting new ip...          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    sxswnui sxsw apple defining language touch different dialects becoming smaller                                \n",
      "1    learning ab google doodles doodles light funny amp innovative exceptions significant occasions gogledodle sxsw\n",
      "2    one inyourface ex stealing show yrs atuser quotat sxsw apple schools mkt expesquot link                       \n",
      "3    iphone sxsw ap would b pretty awesome n't crash mins extended browsing fuckit ilmakeitwork                    \n",
      "4    line outside apple store austin waiting new ipad sxsw link                                                    \n",
      "5    technews one lone dude awaits ipad appleûªs sxsw store link technews apple ipad sxsw tablets tech            \n",
      "6    sxsw tips prince npr videos toy shopping zuckerberg link sxsw ipad                                            \n",
      "7    nu user atuser new ubersocial iphone ap store includes uberguide sxsw sponsored mashable                      \n",
      "8    free sxsw sampler itunes link fremusic                                                                        \n",
      "9    think might go weekend without seeing ipad case twice sxsw                                                    \n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(train.tweet[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Z02OCV2mcZR6",
    "outputId": "c276feca-2fdd-4d01-cbe0-1adf49bc0d21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    audience q protyping tools use sketchboksharpie pens photoshop balsamic google docs axsure etc myprotype sxsw    \n",
      "1    sxsw send best photos amp videos link citizenjournalism cn irepo photography sxsw cyber iphone                   \n",
      "2    atuser 's pic winning ipad unsix sxsw cc atuser atuser link cont link                                            \n",
      "3    google marisa mayer mobile phone cursor physical location new version map fast real life like sxsw               \n",
      "4    sxsw google maps even cooler thought                                                                             \n",
      "5    atuser front atuser popup store sxsw last night link                                                             \n",
      "6    atuser next life 'm coming back ipad women ca n't discontinue hands thing sxsw                                   \n",
      "7    google celebrating pi day style sxsw link                                                                        \n",
      "8    hm bit weird sxsw tending google circle                                                                          \n",
      "9    atuser launch 'circles ' later today sxsw gota love sxsw one platform everything independent film innovative tech\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(test.tweet[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>sxswnui apple defining language touch differen...</td>\n",
       "      <td>1</td>\n",
       "      <td>sxswnui appl defin languag touch differ dialec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>learning ab google doodles doodles light funny...</td>\n",
       "      <td>1</td>\n",
       "      <td>learn ab googl doodl doodl light funni amp inn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one inyourface ex stealing show yrs atuser quo...</td>\n",
       "      <td>2</td>\n",
       "      <td>one inyourfac ex steal show yr atus quotat app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>iphone ap would b pretty awesome n't crash ext...</td>\n",
       "      <td>0</td>\n",
       "      <td>iphon ap would b pretti awesom n't crash exten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>line outside apple store austin waiting new ip...</td>\n",
       "      <td>1</td>\n",
       "      <td>line outsid appl store austin wait new ipad link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment  \\\n",
       "0      1701  sxswnui apple defining language touch differen...          1   \n",
       "1      1851  learning ab google doodles doodles light funny...          1   \n",
       "2      2689  one inyourface ex stealing show yrs atuser quo...          2   \n",
       "3      4525  iphone ap would b pretty awesome n't crash ext...          0   \n",
       "4      3604  line outside apple store austin waiting new ip...          1   \n",
       "\n",
       "                                     tokenized_tweet  \n",
       "0  sxswnui appl defin languag touch differ dialec...  \n",
       "1  learn ab googl doodl doodl light funni amp inn...  \n",
       "2  one inyourfac ex steal show yr atus quotat app...  \n",
       "3  iphon ap would b pretti awesom n't crash exten...  \n",
       "4   line outsid appl store austin wait new ipad link  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "train['tokenized_tweet'] = [nltk.word_tokenize(x) for x in train['tweet']]\n",
    "stemmer = PorterStemmer()\n",
    "train['tokenized_tweet'] = train['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "train['tokenized_tweet'] = train['tokenized_tweet'].apply(lambda x: ' '.join(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7506</td>\n",
       "      <td>audience q protyping tools use sketchboksharpi...</td>\n",
       "      <td>audienc q protyp tool use sketchboksharpi pen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7992</td>\n",
       "      <td>send best photos amp videos link citizenjourna...</td>\n",
       "      <td>send best photo amp video link citizenjourn cn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>atuser 's pic winning ipad unsix cc atuser atu...</td>\n",
       "      <td>atus 's pic win ipad unsix cc atus atus link c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7688</td>\n",
       "      <td>google marisa mayer mobile phone cursor physic...</td>\n",
       "      <td>googl marisa mayer mobil phone cursor physic l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>google maps even cooler thought</td>\n",
       "      <td>googl map even cooler thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  \\\n",
       "0      7506  audience q protyping tools use sketchboksharpi...   \n",
       "1      7992  send best photos amp videos link citizenjourna...   \n",
       "2       247  atuser 's pic winning ipad unsix cc atuser atu...   \n",
       "3      7688  google marisa mayer mobile phone cursor physic...   \n",
       "4      3294                    google maps even cooler thought   \n",
       "\n",
       "                                     tokenized_tweet  \n",
       "0  audienc q protyp tool use sketchboksharpi pen ...  \n",
       "1  send best photo amp video link citizenjourn cn...  \n",
       "2  atus 's pic win ipad unsix cc atus atus link c...  \n",
       "3  googl marisa mayer mobil phone cursor physic l...  \n",
       "4                      googl map even cooler thought  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tokenized_tweet'] = [nltk.word_tokenize(x) for x in test['tweet']]\n",
    "stemmer = PorterStemmer()\n",
    "test['tokenized_tweet'] = test['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "test['tokenized_tweet'] = test['tokenized_tweet'].apply(lambda x: ' '.join(x))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yY2YSsXJcc1j",
    "outputId": "ac990da6-b007-40d0-8c31-c3a7b55457e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7273, 4)\n",
      "(1819, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "rcnrci2Gc8Hh",
    "outputId": "bc5d2d05-3aed-43c1-8205-379109ad2ec2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oh_ZjoZ5fPeY"
   },
   "outputs": [],
   "source": [
    "X = train['tokenized_tweet']\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahzar\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['at_user', 'url'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words=['english',\"AT_USER\",\"URL\",\"rt\",\"link\",\"hashtags\",\"amp\",\"gt\",\"quot\",\"sxsw\",\"bit\",\"ly\"], ngram_range=(1,2))\n",
    "X = cv.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "S5oEp8OLu5cH",
    "outputId": "e589de1d-b310-4a13-e4f7-5304338f1074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7273, 38606)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "sfBL8X16gkJ5",
    "outputId": "75aaa044-a907-4bc9-f34c-e06c94bd465d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "Iob_mGt4halN",
    "outputId": "7b7664a1-1251-46d7-e307-17a3f5cefcc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883593033913841\n",
      "0.6805236887762856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.38      0.44       135\n",
      "           1       0.75      0.80      0.77      1327\n",
      "           2       0.60      0.56      0.58       693\n",
      "           3       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.69      2182\n",
      "   macro avg       0.47      0.43      0.45      2182\n",
      "weighted avg       0.68      0.69      0.68      2182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=77, class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='weighted'))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6814848762603116\n",
      "0.6728002003994007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.36      0.43       135\n",
      "           1       0.74      0.81      0.77      1327\n",
      "           2       0.59      0.53      0.56       693\n",
      "           3       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.68      2182\n",
      "   macro avg       0.47      0.43      0.44      2182\n",
      "weighted avg       0.67      0.68      0.67      2182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "tl=TomekLinks(random_state=42)\n",
    "X_sample1, y_sample1 = tl.fit_sample(X_train, y_train)\n",
    "lr.fit(X_sample1, y_sample1)\n",
    "y_pred1 = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(f1_score(y_test, y_pred1, average='weighted'))\n",
    "print(classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "C-NHJoSQj27F",
    "outputId": "d1f7ad38-dc2e-472b-efe8-9b14bc22863a"
   },
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler(random_state=77)\n",
    "# log = LogisticRegression(random_state=77)\n",
    "# X_sample1, y_sample1 = ros.fit_sample(X_train, y_train)\n",
    "# log.fit(X_sample1, y_sample1)\n",
    "# y_pred1 = log.predict(X_test)\n",
    "# print(accuracy_score(y_test, y_pred1))\n",
    "# print(f1_score(y_test, y_pred1, average='weighted'))\n",
    "# print(classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zitud3pYvrcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7273, 38813)\n",
      "(1819, 38813)\n"
     ]
    }
   ],
   "source": [
    "c2_train=cv.fit_transform(train.tokenized_tweet).toarray()\n",
    "print(c2_train.shape)\n",
    "c2_test=cv.transform(test.tokenized_tweet).toarray()\n",
    "print(c2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NO07WESv6Ap"
   },
   "outputs": [],
   "source": [
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgyrNMOgc9hy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log = LogisticRegression(random_state=77, class_weight='balanced')\n",
    "#X_sample3, y_sample3 = tl.fit_sample(c2_train, y)\n",
    "log.fit(c2_train, y)\n",
    "y_pred3 = log.predict(c2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(random_state=77, class_weight='balanced')\n",
    "log.fit(c2_train, y)\n",
    "y_pred3 = log.predict(c2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDzPE3m4cjbb"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"tweet_id\": test[\"tweet_id\"],\"sentiment\": y_pred3})\n",
    "submission.to_csv((r'C:\\Users\\1403330\\Desktop\\tweets hackathon\\data\\sample_submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Sentiment Analysis Part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
